<!-- Header with typing animation -->
<div align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&size=28&duration=3000&pause=1000&color=2F81F7&center=true&vCenter=true&width=600&lines=AI+Safety+Research+%26+Development;Systematic+Misalignment+Detection;Building+Safety+Testing+Frameworks;Auckland%2C+New+Zealand" alt="Typing Animation" />
</div>

<div align="center">

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/maalonamafaufau/)
[![Website](https://img.shields.io/badge/Research-approxiomresearch.com-green?style=for-the-badge&logo=web)](https://approxiomresearch.com)
[![Location](https://img.shields.io/badge/Location-Auckland%2C%20NZ-red?style=for-the-badge&logo=location)]()

</div>

---

## üî¨ About Me

Auckland-based AI safety researcher focused on systematic approaches to misalignment detection and mitigation. **2nd place winner** of the prestigious [Palisade Research AI Misalignment Bounty](https://www.approxiomresearch.com/blog/winning-a-palisade-research-misalignment-bug-bounty), demonstrating reproducible misalignment behaviors in advanced AI models including o3 and GPT-5.

Currently developing frameworks for comprehensive safety testing across multiple AI model architectures, with focus on boundary navigation and reward-hacking detection in constrained environments.

## Current Research

**[Unified AI Misalignment Framework](https://github.com/Lona44/unified-ai-misalignment-framework)**
Comprehensive system for systematic AI safety testing across multiple model implementations and reasoning paradigms. Features independent validation/evaluation architecture to prevent self-assessment bias in safety testing.

**[LLM RAG Prompt Injections](https://github.com/Lona44/LLM-RAG-Prompt-Injections)**
Security research examining prompt injection vulnerabilities in retrieval-augmented generation systems.

## Technical Focus

- **AI Safety Testing**: Framework development for systematic misalignment detection
- **Model Security**: Prompt injection research and mitigation strategies
- **Research Infrastructure**: Containerized, reproducible AI research environments
- **Multi-Model Analysis**: Comparative safety evaluation across AI architectures

## Technical Stack

**Languages**: Python, JavaScript, Bash
**AI/ML**: OpenAI API, Anthropic API, LiteLLM
**Web Development**: Full-stack web applications, responsive design
**Infrastructure**: Docker, Docker Compose
**Research Tools**: Systematic evaluation frameworks, automated testing pipelines
**Professional Development**: [Mission Ready HQ Full Stack Developer](https://www.credential.net/6f647d31-cbd0-4b57-ac98-9a658d993c31) (August 2025)

## Research Context

Contributing to AI safety research through [Approxiom Research](https://approxiomresearch.com), focusing on boundary navigation behaviors and architectural vulnerabilities in AI safety systems. Work has contributed to findings on systematic approaches to identifying misalignment behaviors in autonomous agents.

## üèÜ Research Achievements

**[Palisade Research Misalignment Bounty - 2nd Place Winner](https://www.approxiomresearch.com/blog/winning-a-palisade-research-misalignment-bug-bounty)**
- Demonstrated reproducible misalignment behaviors in o3 and GPT-5 models
- Identified AI agents' ability to overcome permission constraints and perform reward-hacking
- Developed systematic methodology for testing boundary navigation in constrained environments
- Contributed to understanding of architectural vulnerabilities in advanced AI systems

## Recent Technical Contributions

- Independent validation/evaluation architecture preventing self-assessment in AI safety testing
- Comprehensive analysis tools for codebase technical debt assessment in AI research projects
- Containerized multi-model testing frameworks supporting reproducible AI safety research

## üõ† Technical Expertise

<div align="center">

![Award Winner](https://img.shields.io/badge/üèÜ_Palisade_Research-2nd_Place_Winner-FFD700?style=for-the-badge&logoColor=white)
![Full Stack](https://img.shields.io/badge/üìú_Mission_Ready_HQ-Full_Stack_Developer-4CAF50?style=for-the-badge&logoColor=white)
![Python](https://img.shields.io/badge/Python-Expert-3776AB?style=for-the-badge&logo=python&logoColor=white)
![AI Safety](https://img.shields.io/badge/AI_Safety-Research-FF6B6B?style=for-the-badge&logo=brain&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Proficient-2496ED?style=for-the-badge&logo=docker&logoColor=white)

</div>

---

<div align="center">

### ü§ù Let's Connect

[![Research Website](https://img.shields.io/badge/üî¨_Research-approxiomresearch.com-4A90E2?style=for-the-badge)](https://approxiomresearch.com)
[![Email](https://img.shields.io/badge/üìß_Contact-Get_In_Touch-EA4335?style=for-the-badge)](mailto:your-email@domain.com)

**Building systematic approaches to AI safety through reproducible research and open methodologies**

</div>
